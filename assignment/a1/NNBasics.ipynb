{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "474qPevrYXH8"
      },
      "source": [
        "# Neural Network Basics\n",
        "\n",
        "\n",
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/datasci-w266/2025-spring-main/blob/master/assignment/a1/NNBasics.ipynb)\n",
        "\n",
        "### Brief Review of Machine Learning\n",
        "\n",
        "In supervised learning, parametric models are those where the model is a function of a fixed form with a number of unknown _parameters_.  Together with a loss function and a training set, an optimizer can select parameters to minimize the loss with respect to the training set.  Common optimizers include stochastic gradient descent.  It tweaks the parameters slightly to move the loss \"downhill\" due to a small batch of examples from the training set.\n",
        "\n",
        "## Part A:  Linear & Logistic Regression\n",
        "\n",
        "You've likely seen linear regression before.  In linear regression, we fit a line (technically, hyperplane) that predicts a target variable, $y$, based on some features $x$.  The form of this model is affine (even if we call it \"linear\"):  \n",
        "\n",
        "$$y_{hat} = xW + b$$\n",
        "\n",
        "where $W$ and $b$ are weights and an offset, respectively, and are the parameters of this parametric model.  The loss function that the optimizer uses to fit these parameters is the squared error ($||\\cdots||_2$) between the prediction and the ground truth in the training set.\n",
        "\n",
        "You've also likely seen logistic regression, which is tightly related to linear regression.  Logistic regression also fits a line - this time separating the positive and negative examples of a binary classifier.  The form of this model is similar:\n",
        "\n",
        "$$y_{hat} = \\sigma(xW + b)$$\n",
        "\n",
        "where again $W$ and $b$ are the parameters of this model, and $\\sigma$ is the [sigmoid function](https://en.wikipedia.org/wiki/Sigmoid_function) which maps un-normalized scores (\"logits\") to values $\\hat{y} \\in [0,1]$ that represent probabilities. The loss function that the optimizer uses to fit these parameters is the [cross entropy](../../materials/lesson_notebook/lesson_1_NN_Review.ipynb) between the prediction and the ground truth in the training set.\n",
        "\n",
        "This pattern of an affine transform, $xW + b$, occurs over and over in machine learning.\n",
        "\n",
        "**We'll use logistic regression as our running example for the rest of this part.**\n",
        "\n",
        "\n",
        "### Short Answer Questions\n",
        "\n",
        "Imagine you want to implement logistic regression:\n",
        "\n",
        "* `z = xW + b`\n",
        "* `y_hat = sigmoid(z)`\n",
        "\n",
        "Where:\n",
        "1.  `x` is an 11-dimensional feature vector\n",
        "2.  `W` is the weight vector\n",
        "3.  `b` is the bias term\n",
        "\n",
        "What are the dimensions of `W` and `b`?  Recall that in logistic regression, `z` is just a scalar (commonly referred to as the \"logit\").\n",
        "\n",
        "Sketch a picture of the whole equation using rectangles to illustrate the dimensions of `x`, `W`, and `b`.  See examples below for inspiration (though please label each dimension).  We don't ask you to submit this, but make sure you can do it!  It's the \"print\" debugging statement of neural networks!  It's also useful for reading papers... if you can't draw the shapes of all the tensors, you don't (yet) know what's going on!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B93Z5ZbxYXH_"
      },
      "source": [
        "## Part B: Batching\n",
        "\n",
        "Let's say we want to perform inference using your model (parameters `W` and `b`) above on multiple examples instead of just one. On modern hardware (especially GPUs), we can do this efficiently by *batching*.\n",
        "\n",
        "To do this, we stack up the feature vectors in x like in the diagram below.  Note that changing the number of examples you run on (i.e. your batch size) *does not* affect the number of parameters in your model.  You're just running the same thing in parallel (instead of running the above one feature vector at a time at a time).\n",
        "\n",
        "![](https://github.com/datasci-w266/2025-spring-main/blob/master/assignment/a1/batchaffine.png?raw=1)\n",
        "\n",
        "The red (# features) and blue (batch size) lines represent dimensions that are the same.\n",
        "\n",
        "### Short Answer Questions\n",
        "\n",
        "If we have 11 features and running the model in parallel with 30 examples, what are the dimensions of:\n",
        "\n",
        "1. `W` ?\n",
        "2. `b` ?\n",
        "3. `x` ?\n",
        "4. `z` ?\n",
        "\n",
        "_Hint:_ remember that your model parameters stay fixed!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Is8PIY0cYXIA"
      },
      "source": [
        "## Part C: Logistic Regression - NumPy Implementation\n",
        "\n",
        "In this section, we'll implement logistic regression by hand and compute a few values to make sure we understand what's going on!\n",
        "\n",
        "Let's say your model has the following parameters:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "kfL7B6RFYXIB"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "W = np.array([45,6,3,25,-1])\n",
        "b = 5"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JdtKDbeGYXIE"
      },
      "source": [
        "If you want to run the model on the following three examples:\n",
        "\n",
        "* [1, 2, 3, 4, 5]\n",
        "* [0, 0, 0, 0, 5]\n",
        "* [-3, -4, -12, -1, 1]\n",
        "\n",
        "Construct the x matrix **such that you compute the answer all in one big batch** and compute the probability of the positive class for each."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OhhWS8YJYXIF",
        "outputId": "4c74a7ba-b4c8-42b3-9555-ac61fd4c0046"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Logits (z): [ 166    0 -216]\n",
            "Probabilities (y_hat): [1.00000000e+00 5.00000000e-01 1.55737037e-94]\n"
          ]
        }
      ],
      "source": [
        "# Import sigmoid.\n",
        "from scipy.special import expit as sigmoid\n",
        "\n",
        "# Parameters\n",
        "W = np.array([45, 6, 3, 25, -1])\n",
        "b = 5\n",
        "\n",
        "# Input examples stacked as rows in a matrix (batch)\n",
        "x = np.array([\n",
        "    [1, 2, 3, 4, 5],\n",
        "    [0, 0, 0, 0, 5],\n",
        "    [-3, -4, -12, -1, 1]\n",
        "])\n",
        "\n",
        "# Calculate logits (z = xW + b)\n",
        "z = np.dot(x, W) + b\n",
        "\n",
        "# Apply sigmoid to get probabilities\n",
        "y_hat = sigmoid(z)\n",
        "\n",
        "# Print results\n",
        "print(\"Logits (z):\", z)\n",
        "print(\"Probabilities (y_hat):\", y_hat)\n",
        "\n",
        "### END YOUR CODE"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JD2ntIUMYXIG"
      },
      "source": [
        "### Short Answer Questions\n",
        "\n",
        "1. What is the probability of the positive class for the second (middle) example?\n",
        "# Output: y_hat[1]\n",
        "\n",
        "2. What is the cross-entropy loss in Base 2 of the second example if its label is positive?\n",
        "# Formula:\n",
        "‚àí\n",
        "log\n",
        "‚Å°\n",
        "(\n",
        "ùë¶\n",
        "hat\n",
        ")\n",
        "‚àílog(y\n",
        "hat\n",
        "‚Äã\n",
        " ) for positive class\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mPauIVl_YXIG"
      },
      "source": [
        "## Part D: NumPy Feed Forward Neural Network\n",
        "\n",
        "Let's do the same procedure for a simple feed-forward neural network.\n",
        "\n",
        "Imagine you have a 3 layer network (hint: # of affines = # of layers. The affine is the W + b part of a layer).  Each hidden layer is size 10.  Just like before, you've already trained your model and you just want to run it forward.  For this exercise, let's say that each weight matrix is np.ones(...) and each bias term is [-1, -2, -3, ..., -n] if the bias term is $n$ long.  Compute the probability of the positive class for the three examples above, again in a single batch.\n",
        "\n",
        "**Hint:  Draw the shapes of the matrices at each layer out on a piece of paper!  Include it with any questions you post to Ed Discussion.**\n",
        "\n",
        "Assume your model uses a sigmoid as the nonlinearity for all layers."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4yCxHOxNYXIH",
        "outputId": "1e0f079f-3530-4140-ceae-f1786f0a49f5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Final Probabilities (y_hat): [[0.99908589]\n",
            " [0.99908529]\n",
            " [0.14088356]]\n"
          ]
        }
      ],
      "source": [
        "# Input examples\n",
        "x = np.array([\n",
        "    [1, 2, 3, 4, 5],\n",
        "    [0, 0, 0, 0, 5],\n",
        "    [-3, -4, -12, -1, 1]\n",
        "])\n",
        "\n",
        "# Initialize parameters for the 3-layer network\n",
        "# Layer 1\n",
        "W1 = np.ones((5, 10))\n",
        "b1 = np.array([-1] * 10)\n",
        "\n",
        "# Layer 2\n",
        "W2 = np.ones((10, 10))\n",
        "b2 = np.array([-2] * 10)\n",
        "\n",
        "# Layer 3\n",
        "W3 = np.ones((10, 1))\n",
        "b3 = np.array([-3])\n",
        "\n",
        "# Forward pass\n",
        "z1 = np.dot(x, W1) + b1\n",
        "a1 = sigmoid(z1)\n",
        "\n",
        "z2 = np.dot(a1, W2) + b2\n",
        "a2 = sigmoid(z2)\n",
        "\n",
        "z3 = np.dot(a2, W3) + b3\n",
        "y_hat = sigmoid(z3)\n",
        "\n",
        "# Print the final probabilities\n",
        "print(\"Final Probabilities (y_hat):\", y_hat)\n",
        "\n",
        "### END YOUR CODE"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ql46cUQAYXIH"
      },
      "source": [
        "### Short Answer Questions\n",
        "\n",
        "1.  What is the probability of the third example?\n",
        "y_hat[2]\n",
        "\n",
        "2.  What is the cross-entropy loss if its label is negative? 0.152\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JvUIzz8bYXIH"
      },
      "source": [
        "## Part E: Softmax\n",
        "\n",
        "Recall that softmax(z) is a vector with the same length as z, and whose components are:  $softmax(z)_i = \\frac{e^{z_i}}{\\Sigma_j e^{z_j}}$.\n",
        "\n",
        "### Short Answer Questions\n",
        "\n",
        "1. If the logits coming from the main body of the network are [4, 6, 8], what is the probability of the middle class? 0.117\n",
        "2. What is the cross-entropy loss if the correct class is the last one? (i.e. corresponding to logit=8)? 0.143\n",
        "3. If you had such a three-class classification problem, what would the dimensions of W and b be for the last layer of the feed forward neural network above?\n",
        "W: (hidden_layer_size, 3) (where hidden_layer_size is the number of neurons in the previous layer).\n",
        "b: (3,) (one bias for each class)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4PLgBnapYXII",
        "outputId": "cb516a9d-f868-49c4-eac2-40597dc9ad72"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Softmax Probabilities: [0.01587624 0.11731043 0.86681333]\n",
            "Probability of the middle class: 0.11731042782619837\n",
            "Cross-Entropy Loss for correct class: 0.1429316284998995\n"
          ]
        }
      ],
      "source": [
        "# Logits\n",
        "logits = np.array([4, 6, 8])\n",
        "\n",
        "# Compute softmax\n",
        "softmax = np.exp(logits) / np.sum(np.exp(logits))\n",
        "print(\"Softmax Probabilities:\", softmax)\n",
        "\n",
        "# Probability of the middle class\n",
        "print(\"Probability of the middle class:\", softmax[1])\n",
        "\n",
        "# Cross-entropy loss for the last class (logit=8)\n",
        "loss = -np.log(softmax[2])\n",
        "print(\"Cross-Entropy Loss for correct class:\", loss)\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}